{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN Demo\n",
    "A quick demo of InfoGAN\n",
    "\n",
    "## Packages & Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from InfoGAN import Generator, DiscriminatorFrontEnd, DiscriminatorBackend,DiscriminatorInfo\n",
    "from util import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 200\n",
    "USE_GPU = True\n",
    "\n",
    "DISPLAY_STEP = 100\n",
    "PLOT_EPOCH = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "Set Download to True if MNIST is not availible on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_dataset = dset.MNIST(root='./data/',transform=transforms.ToTensor(),download=False)\n",
    "dataloader = DataLoader(mnist_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction\n",
    "\n",
    "For details about model implementation, please refer to `InfoGAN.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = DiscriminatorFrontEnd()\n",
    "G = Generator()\n",
    "TF = DiscriminatorBackend()\n",
    "Q = DiscriminatorInfo()\n",
    "\n",
    "# Loss function\n",
    "D_criterion = nn.BCEWithLogitsLoss()\n",
    "Q_discr_criterion = nn.CrossEntropyLoss()\n",
    "Q_conti_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "if USE_GPU:\n",
    "    D = D.cuda()\n",
    "    G = G.cuda()\n",
    "    TF = TF.cuda()\n",
    "    Q = Q.cuda()\n",
    "    D_criterion = D_criterion.cuda()\n",
    "    Q_discr_criterion = Q_discr_criterion.cuda()\n",
    "    Q_conti_criterion = Q_conti_criterion.cuda()\n",
    "\n",
    "optimD = optim.Adam([{'params':D.parameters()}, {'params':TF.parameters()}], lr=0.0002)\n",
    "optimG = optim.Adam([{'params':G.parameters()}, {'params':Q.parameters()}], lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Progress\n",
    "\n",
    "Training Log will be stored as `infogan.log` under `InfoGAN_pytorch/`\n",
    "\n",
    "The result of generator with fixed 10-class input along with 1 countinous latent code assigned in range(-2,2,0.5)\n",
    "\n",
    "will be stored under `InfoGAN_pytorch/` automatically every 5 epochs.\n",
    "\n",
    "\n",
    "\n",
    "## WorkFlow\n",
    "### Step 1. Training Discriminator\n",
    "   \n",
    "   - Sample random noise to create fake image\n",
    "   - Train Discriminator with Real image from MNIST and Fake image from Generator\n",
    "\n",
    "### Step 2. Training Generator and Q\n",
    "\n",
    "   - Update Generator with fixed D, loss = D's classification error\n",
    "   - According to the paper proposing InfoGAN, jointly update Q together (important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-  1-D_loss-1.1713-GQ_loss-3.3608-Image_loss-0.6297-Disc_loss-2.3405-Conti_loss-0.3906\n",
      "epoch-  2-D_loss-1.2225-GQ_loss-3.3818-Image_loss-0.6563-Disc_loss-2.3370-Conti_loss-0.3885\n",
      "epoch-  3-D_loss-1.2376-GQ_loss-3.4157-Image_loss-0.6763-Disc_loss-2.3325-Conti_loss-0.4069\n",
      "epoch-  4-D_loss-1.2488-GQ_loss-3.4396-Image_loss-0.6769-Disc_loss-2.3362-Conti_loss-0.4265\n",
      "epoch-  5-D_loss-1.2563-GQ_loss-3.4299-Image_loss-0.6776-Disc_loss-2.3356-Conti_loss-0.4167\n",
      "epoch-  6-D_loss-1.2730-GQ_loss-3.4187-Image_loss-0.6764-Disc_loss-2.3423-Conti_loss-0.4000\n",
      "epoch-  7-D_loss-1.2794-GQ_loss-3.4218-Image_loss-0.6761-Disc_loss-2.3418-Conti_loss-0.4040\n",
      "epoch-  8-D_loss-1.2952-GQ_loss-3.4162-Image_loss-0.6749-Disc_loss-2.3407-Conti_loss-0.4007\n",
      "epoch-  9-D_loss-1.3038-GQ_loss-3.4032-Image_loss-0.6754-Disc_loss-2.3406-Conti_loss-0.3871\n",
      "epoch- 10-D_loss-1.3093-GQ_loss-3.4010-Image_loss-0.6745-Disc_loss-2.3396-Conti_loss-0.3869\n",
      "epoch- 11-D_loss-1.3119-GQ_loss-3.4120-Image_loss-0.6756-Disc_loss-2.3439-Conti_loss-0.3924\n",
      "epoch- 12-D_loss-1.3129-GQ_loss-3.4049-Image_loss-0.6761-Disc_loss-2.3460-Conti_loss-0.3828\n",
      "epoch- 13-D_loss-1.3113-GQ_loss-3.4015-Image_loss-0.6769-Disc_loss-2.3452-Conti_loss-0.3794\n",
      "epoch- 14-D_loss-1.3156-GQ_loss-3.4045-Image_loss-0.6767-Disc_loss-2.3467-Conti_loss-0.3811\n",
      "epoch- 15-D_loss-1.3106-GQ_loss-3.4092-Image_loss-0.6770-Disc_loss-2.3446-Conti_loss-0.3875\n",
      "epoch- 16-D_loss-1.3109-GQ_loss-3.4143-Image_loss-0.6770-Disc_loss-2.3499-Conti_loss-0.3874\n",
      "epoch- 17-D_loss-1.3007-GQ_loss-3.4125-Image_loss-0.6788-Disc_loss-2.3442-Conti_loss-0.3895\n",
      "epoch- 18-D_loss-1.3039-GQ_loss-3.4112-Image_loss-0.6778-Disc_loss-2.3510-Conti_loss-0.3824\n",
      "epoch- 19-D_loss-1.2994-GQ_loss-3.4095-Image_loss-0.6787-Disc_loss-2.3508-Conti_loss-0.3801\n",
      "epoch- 20-D_loss-1.2948-GQ_loss-3.4137-Image_loss-0.6799-Disc_loss-2.3495-Conti_loss-0.3843\n",
      "epoch- 21-D_loss-1.2927-GQ_loss-3.4116-Image_loss-0.6800-Disc_loss-2.3474-Conti_loss-0.3842\n",
      "epoch- 22-D_loss-1.2880-GQ_loss-3.4168-Image_loss-0.6811-Disc_loss-2.3474-Conti_loss-0.3883\n",
      "epoch- 23-D_loss-1.2864-GQ_loss-3.4101-Image_loss-0.6816-Disc_loss-2.3466-Conti_loss-0.3819\n",
      "epoch- 24-D_loss-1.2826-GQ_loss-3.4172-Image_loss-0.6823-Disc_loss-2.3476-Conti_loss-0.3873\n",
      "epoch- 25-D_loss-1.2787-GQ_loss-3.4145-Image_loss-0.6834-Disc_loss-2.3469-Conti_loss-0.3841\n",
      "epoch- 26-D_loss-1.2739-GQ_loss-3.4170-Image_loss-0.6844-Disc_loss-2.3473-Conti_loss-0.3854\n",
      "epoch- 27-D_loss-1.2734-GQ_loss-3.4125-Image_loss-0.6838-Disc_loss-2.3484-Conti_loss-0.3803\n",
      "epoch- 28-D_loss-1.2719-GQ_loss-3.4081-Image_loss-0.6840-Disc_loss-2.3456-Conti_loss-0.3785\n",
      "epoch- 29-D_loss-1.2656-GQ_loss-3.4110-Image_loss-0.6858-Disc_loss-2.3471-Conti_loss-0.3781\n",
      "epoch- 30-D_loss-1.2652-GQ_loss-3.4156-Image_loss-0.6855-Disc_loss-2.3485-Conti_loss-0.3815\n",
      "epoch- 31-D_loss-1.2618-GQ_loss-3.4163-Image_loss-0.6858-Disc_loss-2.3478-Conti_loss-0.3827\n",
      "epoch- 32-D_loss-1.2618-GQ_loss-3.4141-Image_loss-0.6854-Disc_loss-2.3464-Conti_loss-0.3823\n",
      "epoch- 33-D_loss-1.2577-GQ_loss-3.4120-Image_loss-0.6869-Disc_loss-2.3446-Conti_loss-0.3805\n",
      "epoch- 34-D_loss-1.2580-GQ_loss-3.4120-Image_loss-0.6866-Disc_loss-2.3447-Conti_loss-0.3808\n",
      "epoch- 35-D_loss-1.2556-GQ_loss-3.4113-Image_loss-0.6867-Disc_loss-2.3435-Conti_loss-0.3811\n",
      "epoch- 36-D_loss-1.2535-GQ_loss-3.4126-Image_loss-0.6865-Disc_loss-2.3465-Conti_loss-0.3797\n",
      "epoch- 37-D_loss-1.2508-GQ_loss-3.4130-Image_loss-0.6871-Disc_loss-2.3449-Conti_loss-0.3810\n",
      "epoch- 38-D_loss-1.2485-GQ_loss-3.4117-Image_loss-0.6873-Disc_loss-2.3474-Conti_loss-0.3770\n",
      "epoch- 39-D_loss-1.2517-GQ_loss-3.4125-Image_loss-0.6868-Disc_loss-2.3503-Conti_loss-0.3755\n",
      "epoch- 40-D_loss-1.2445-GQ_loss-3.4104-Image_loss-0.6873-Disc_loss-2.3476-Conti_loss-0.3755\n",
      "epoch- 41-D_loss-1.2477-GQ_loss-3.4109-Image_loss-0.6860-Disc_loss-2.3487-Conti_loss-0.3762\n",
      "epoch- 42-D_loss-1.2400-GQ_loss-3.4098-Image_loss-0.6871-Disc_loss-2.3483-Conti_loss-0.3744\n",
      "epoch- 43-D_loss-1.2400-GQ_loss-3.4118-Image_loss-0.6867-Disc_loss-2.3483-Conti_loss-0.3768\n",
      "epoch- 44-D_loss-1.2354-GQ_loss-3.4089-Image_loss-0.6872-Disc_loss-2.3488-Conti_loss-0.3730\n",
      "epoch- 45-D_loss-1.2355-GQ_loss-3.4181-Image_loss-0.6864-Disc_loss-2.3501-Conti_loss-0.3816\n",
      "epoch- 46-D_loss-1.2327-GQ_loss-3.4141-Image_loss-0.6869-Disc_loss-2.3498-Conti_loss-0.3774\n",
      "epoch- 47-D_loss-1.2278-GQ_loss-3.4162-Image_loss-0.6868-Disc_loss-2.3492-Conti_loss-0.3802\n",
      "epoch- 48-D_loss-1.2263-GQ_loss-3.4110-Image_loss-0.6867-Disc_loss-2.3469-Conti_loss-0.3774\n",
      "epoch- 49-D_loss-1.2233-GQ_loss-3.4061-Image_loss-0.6856-Disc_loss-2.3461-Conti_loss-0.3744\n",
      "epoch- 50-D_loss-1.2224-GQ_loss-3.4114-Image_loss-0.6855-Disc_loss-2.3490-Conti_loss-0.3769\n",
      "epoch- 51-D_loss-1.1554-GQ_loss-1.5409-Image_loss-0.6905-Disc_loss-0.7030-Conti_loss-0.1474\n",
      "epoch- 52-D_loss-1.1015-GQ_loss-0.7122-Image_loss-0.6915-Disc_loss-0.0036-Conti_loss-0.0171\n",
      "epoch- 53-D_loss-1.1021-GQ_loss-0.7063-Image_loss-0.6907-Disc_loss-0.0025-Conti_loss-0.0132\n",
      "epoch- 54-D_loss-1.1037-GQ_loss-0.7050-Image_loss-0.6907-Disc_loss-0.0021-Conti_loss-0.0122\n",
      "epoch- 55-D_loss-1.0981-GQ_loss-0.7039-Image_loss-0.6912-Disc_loss-0.0014-Conti_loss-0.0114\n",
      "epoch- 56-D_loss-1.0950-GQ_loss-0.7036-Image_loss-0.6915-Disc_loss-0.0013-Conti_loss-0.0108\n",
      "epoch- 57-D_loss-1.0955-GQ_loss-0.7029-Image_loss-0.6913-Disc_loss-0.0011-Conti_loss-0.0105\n",
      "epoch- 58-D_loss-1.0946-GQ_loss-0.7028-Image_loss-0.6913-Disc_loss-0.0011-Conti_loss-0.0104\n",
      "epoch- 59-D_loss-1.0968-GQ_loss-0.7029-Image_loss-0.6908-Disc_loss-0.0012-Conti_loss-0.0109\n",
      "epoch- 60-D_loss-1.0939-GQ_loss-0.7019-Image_loss-0.6912-Disc_loss-0.0008-Conti_loss-0.0098\n",
      "epoch- 61-D_loss-1.0910-GQ_loss-0.7024-Image_loss-0.6916-Disc_loss-0.0009-Conti_loss-0.0099\n",
      "epoch- 62-D_loss-1.0904-GQ_loss-0.7023-Image_loss-0.6917-Disc_loss-0.0007-Conti_loss-0.0099\n",
      "epoch- 63-D_loss-1.0887-GQ_loss-0.7008-Image_loss-0.6918-Disc_loss-0.0005-Conti_loss-0.0085\n",
      "epoch- 64-D_loss-1.0903-GQ_loss-0.7012-Image_loss-0.6915-Disc_loss-0.0005-Conti_loss-0.0092\n",
      "epoch- 65-D_loss-1.0904-GQ_loss-0.7020-Image_loss-0.6915-Disc_loss-0.0009-Conti_loss-0.0096\n",
      "epoch- 66-D_loss-1.0912-GQ_loss-0.7015-Image_loss-0.6914-Disc_loss-0.0006-Conti_loss-0.0095\n",
      "epoch- 67-D_loss-1.0927-GQ_loss-0.7013-Image_loss-0.6911-Disc_loss-0.0006-Conti_loss-0.0096\n",
      "epoch- 68-D_loss-1.0938-GQ_loss-0.7013-Image_loss-0.6907-Disc_loss-0.0007-Conti_loss-0.0099\n",
      "epoch- 69-D_loss-1.0972-GQ_loss-0.7014-Image_loss-0.6902-Disc_loss-0.0007-Conti_loss-0.0106\n",
      "epoch- 70-D_loss-1.0905-GQ_loss-0.7026-Image_loss-0.6912-Disc_loss-0.0010-Conti_loss-0.0103\n",
      "epoch- 71-D_loss-1.0930-GQ_loss-0.7014-Image_loss-0.6908-Disc_loss-0.0007-Conti_loss-0.0099\n",
      "epoch- 72-D_loss-1.0933-GQ_loss-0.7011-Image_loss-0.6909-Disc_loss-0.0006-Conti_loss-0.0097\n",
      "epoch- 73-D_loss-1.0945-GQ_loss-0.7010-Image_loss-0.6909-Disc_loss-0.0005-Conti_loss-0.0096\n",
      "epoch- 74-D_loss-1.0895-GQ_loss-0.7010-Image_loss-0.6914-Disc_loss-0.0005-Conti_loss-0.0091\n",
      "epoch- 75-D_loss-1.0942-GQ_loss-0.7008-Image_loss-0.6906-Disc_loss-0.0008-Conti_loss-0.0093\n",
      "epoch- 76-D_loss-1.0939-GQ_loss-0.7009-Image_loss-0.6908-Disc_loss-0.0006-Conti_loss-0.0095\n",
      "epoch- 77-D_loss-1.0960-GQ_loss-0.7006-Image_loss-0.6903-Disc_loss-0.0005-Conti_loss-0.0097\n",
      "epoch- 78-D_loss-1.0943-GQ_loss-0.7010-Image_loss-0.6908-Disc_loss-0.0005-Conti_loss-0.0096\n",
      "epoch- 79-D_loss-1.0971-GQ_loss-0.7010-Image_loss-0.6902-Disc_loss-0.0008-Conti_loss-0.0100\n",
      "epoch- 80-step-300-D_loss-1.093169-GQ_loss-0.7004\r"
     ]
    }
   ],
   "source": [
    "training_message = 'epoch-{:3}-step-{:3}-D_loss-{:.6f}-GQ_loss-{:.4f}'\n",
    "epoch_end_message = 'epoch-{:3}-D_loss-{:.4f}-GQ_loss-{:.4f}-Image_loss-{:.4f}-Disc_loss-{:.4f}-Conti_loss-{:.4f}'\n",
    "\n",
    "log = open('infogan.log','w')\n",
    "log_message = '{:.4f},{:.4f},{:.4f},{:.4f}\\n'\n",
    "log.write('D_loss,G_loss,Disc_loss,Conti_loss\\n')\n",
    "\n",
    "\n",
    "demo_z1, demo_z2 = get_test_noise()\n",
    "demo_z1 = Variable(demo_z1.cuda()) if USE_GPU else Variable(demo_z1)\n",
    "demo_z2 = Variable(demo_z2.cuda()) if USE_GPU else Variable(demo_z2)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Output Demo\n",
    "    if (epoch%PLOT_EPOCH) == 0:\n",
    "        save_fig(demo_z1,G,'fig/wiegthed_z1_epoch{}.jpg'.format(epoch))\n",
    "        save_fig(demo_z2,G,'fig/wiegthed_z2_epoch{}.jpg'.format(epoch))\n",
    "\n",
    "    D_loss = 0 # Accumalate loss of D\n",
    "    G_loss = 0 # Accumalate loss of G\n",
    "    Q_loss_dis = 0 # Accumalate loss of Q (discrete)\n",
    "    Q_loss_conti = 0 # Accumalate loss of Q (continuous)\n",
    "    for step,batch_data in enumerate(dataloader):\n",
    "        batch_size = batch_data[0].size(0)\n",
    "        \n",
    "        # Step 1.\n",
    "        optimD.zero_grad()\n",
    "        ### Real Images\n",
    "        real_x = batch_data[0].cuda() if USE_GPU else batch_data[0]\n",
    "        real_x = Variable(real_x)\n",
    "        conv_feature_1 = D(real_x)\n",
    "        prob_real = TF(conv_feature_1)\n",
    "        real_label = torch.ones(batch_size).cuda() if USE_GPU else torch.ones(batch_size)\n",
    "        real_label = Variable(real_label.view(-1,1),requires_grad=False)\n",
    "        D_real_loss = D_criterion(prob_real,real_label)\n",
    "        D_real_loss.backward()\n",
    "        ### Fake Images\n",
    "        z, fake_idx = sample_noise(batch_size)\n",
    "        z = Variable(torch.Tensor(z).cuda()) if USE_GPU else Variable(torch.Tensor(z))\n",
    "        fake_x = G(z)\n",
    "        conv_feature_2 = D(fake_x.detach())\n",
    "        prob_fake = TF(conv_feature_2)\n",
    "        real_label = torch.zeros(batch_size).cuda() if USE_GPU else torch.zeros(batch_size)\n",
    "        real_label = Variable(real_label.view(-1,1),requires_grad=False)\n",
    "        D_fake_loss = D_criterion(prob_fake,real_label)\n",
    "        D_fake_loss.backward()\n",
    "\n",
    "        D_loss += D_real_loss+D_fake_loss\n",
    "        optimD.step()\n",
    "        \n",
    "        # Step 2.\n",
    "        optimG.zero_grad()\n",
    "        ### Image Reality\n",
    "        conv_feature_3 = D(fake_x)\n",
    "        discriminator_prediction = TF(conv_feature_3)\n",
    "        fake_label = torch.ones(batch_size).cuda() if USE_GPU else torch.ones(batch_size)\n",
    "        fake_label = Variable(fake_label.view(-1,1),requires_grad=False)\n",
    "        generator_loss = D_criterion(discriminator_prediction,fake_label)\n",
    "        G_loss += generator_loss\n",
    "        ### Mutaul Info\n",
    "        pred_c = Q(conv_feature_3)\n",
    "        fake_idx = torch.LongTensor(fake_idx).cuda() if USE_GPU else torch.LongTensor(fake_idx)\n",
    "        fake_idx = Variable(fake_idx,requires_grad=False)\n",
    "        digit_classify_loss = Q_discr_criterion(pred_c[:,:10],fake_idx)\n",
    "        Q_loss_dis += digit_classify_loss\n",
    "        conti_loss = Q_conti_criterion(pred_c[:,10:],z[:,-2:])\n",
    "        Q_loss_conti += conti_loss\n",
    "        \n",
    "        if epoch >= 50:\n",
    "            w1 = 1.0\n",
    "            w2 = 1.0\n",
    "        else:\n",
    "            w1 = 0.0\n",
    "            w2 = 0.0\n",
    "        \n",
    "        G_Q_loss = generator_loss + w1*digit_classify_loss + w2*conti_loss\n",
    "        G_Q_loss.backward()\n",
    "        optimG.step()\n",
    "        \n",
    "        log.write(log_message.format(float((D_real_loss+D_fake_loss).data.cpu().numpy()),\n",
    "                                     float(generator_loss.data.cpu().numpy()),\n",
    "                                     float(digit_classify_loss.data.cpu().numpy()),\n",
    "                                     float(conti_loss.data.cpu().numpy())))\n",
    "        \n",
    "        if step%DISPLAY_STEP == 0:\n",
    "            print(training_message.format(epoch+1,step,float(D_loss.data.cpu().numpy())/(step+1),\n",
    "                                          float((G_loss+Q_loss_dis+Q_loss_conti).data.cpu().numpy())/(step+1)),\n",
    "                 flush=True,end='\\r')\n",
    "    # End of epoch\n",
    "    D_loss = float(D_loss.data.cpu().numpy())/(step+1)\n",
    "    G_loss = float(G_loss.data.cpu().numpy())/(step+1)\n",
    "    Q_loss_dis = float(Q_loss_dis.data.cpu().numpy())/(step+1)\n",
    "    Q_loss_conti = float(Q_loss_conti.data.cpu().numpy())/(step+1)\n",
    "    print(epoch_end_message.format(epoch+1,D_loss,G_loss+Q_loss_dis+Q_loss_conti,G_loss,Q_loss_dis,Q_loss_conti))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
